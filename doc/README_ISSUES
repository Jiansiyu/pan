Comments by R. Holmes, 26 Jul 2002
----------------------------------

With the new standardized file names, the macros that expect to find
pan_%d.root will break.  The quick and dirty fix will be to change
these to parity02_%d.root, but it may be worthwhile to do something
more elegant.


Comments by R. Holmes, 23 Jul 2002
----------------------------------

TaMysql is even more broken, because it needs a GetCutNames
definition. 

(Note, though, that GetCutNumber is defined in VaDataBase in terms of
other database methods -- so that, at least, should work with the SQL
database.) 


Comments by R. Michaels, 23 July 2002
-------------------------------------

After the round of changes to incorporate scalers as alternatives
to ADCs (see the ChangeLog for details), we have the following issues
to fix:

1. TaMysql is broken.  To the extent possible, both it and TaAsciiDB
   classes should use inheritance to minimize the pain of changing the code.
   (That's what C++ is supposed to do !!)

2. scaler calibration needs to be done correctly in TaEvent::Decode

3. Perl script to generate DevTypes.hh is desirable.


Comments by R. Holmes, 16 Apr 2002
----------------------------------

I tried to put in deletes for everything we new in TaRun.cc.  But
there's some weirdness if I try to delete fEvtree.  If I have this
code in Uncreate():

  delete fEvtree;
  delete fCoda;
  delete fEvent;
  delete fDataBase;
  delete fDevices;
  delete fCutList;
  delete fESliceStats;
  delete fERunStats;
  delete fPSliceStats;
  delete fPRunStats;

then it hangs in delete fEvent.  If I comment out delete fEvent then
it hangs in delete fDataBase (I think, or perhaps one of the later
deletes).  But if I comment out delete fEvtree there's no hang.  I
have the feeling I've seen behavior like this before -- thought maybe
it was because I was trying to delete the same allocation twice, but
I've looked into it and I don't think I am.  Strange.

For now, I comment out delete fEvtree.

TaAsciiDB needs fixes to delete its allocations.


Comments by R. Michaels, 6 Apr 2002
-----------------------------------

Studied profiles and effect of gcc optimization level.  Optimization
can lead an approx 10% increase in speed, not more.  Leave the 
default at level 1 as a compromise between speed of compilation
and speed of code.  

Profiled optimized code shows that 42% of the "visible" time is
spent in TaEvent deep copy.  However, the visible time (i.e. cumulative
time reported by gprof) is only 15% of the total; therefore the deep
copy appears to be 6% of the total.  This is consistent with the
observation that by adjusting MAXKEYS in TaEvent, which adjusts the
size of the deep copy and calibrates its influence on the speed,
the deep copy costs about 6% of cpu time.  Unfortunately, when we
profile without optimization, while the total speed doesn't change
much (10% level), the distribution becomes inconsistent with the
above pattern.  The conclusions for now is that we do not know from
gprof what the main bottlenecks are.

See /doc/prof_opt.dat  and  /doc/prof_noopt.dat


Comments by R. Michaels, 5 Apr 2002
-----------------------------------

As we all know, "pan" was horribly slow.  There appear to have
been three main culprits:

  1. Use of string keys in map<string, *> is slow due to
     log(N) lookup time.  hash_map was ~4 times faster, but
     random access of a static array can be ~100 times faster.

  2. TaEvent contained a plethora of data that needed to be
     copied each time an event was added to a queue.  This
     was unecessary because much of this data never changed.

  3. Thousands of calls from Cut related classes, on average
     per event.  I don't know why.

In this release, I have fixed #1 and #2 (see ChangeLog for details),
and at present "pan" runs at 680 Hz on an 800 MHz PC with local I/O.
This is still a bit slower than "apar" but would be acceptable for
feedback at 30 Hz.  I did not fix #3 yet and leave it to Rich.

I noticed that "gprof" cumulative time falls far short of the
cpu time measured with other methods with or without -pg compile
option (e.g. the method of "time pan -f file").  Some of this 
shortfall is due to ROOT not being compiled with -pg.  Also noticed
that the profile distribution depends on g++ optimization, which is 
reasonable, but it indicates we may benefit from learning more about
g++ optimization.

At present there are still some warnings when compiling and
running the code.  I'm too burned out to fix them now; but they
seem harmless.  However, one thing deserving attention is that
BPM10 x,y differences are very different from the previous version
of "pan".  Actually I think the present version makes sense, but
until the difference is understood, I'm nervous.  Strangely, these 
are the only quantities affected.  All other results (asymmetries, 
differences, root tree values, etc), look identical to previuos
version results.


Comments by A. Vacheret, March, 2002
----------------------------------
  
The code uses 32 Mbytes memory at present, I think this is because we have the pair analysis loop
working now. No evident memory leak during a run but Pan is a kind of slow compared to apar. 

Feedback are working but not the cuts yet. With cuts and pedestal calibration, it should 
work like apar.

Statisitics shows error during calculation, didn't take the time to look at this carefully

Will put the type of analysis in the root file name. Also the time parameter for the feedback should be
in the database to be able to change the lenght of a feedback minirun.




Comments by R. Michaels, Feb, 2002
----------------------------------

Several of the problems mentioned below were addressed,
see ChangeLog.

Still some compiler warnings, and you may get in trouble
if you don't 'make clean' before 'make'.

Do we need to distinguish between 'run type' and 'analysis
type' in the database ?  For now I let this distinction exist
and have awkwardly e.g. runtype = anatype = BEAM

